import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader, random_split
import pandas as pd
import os
from torch.utils.data import Dataset
import numpy as np
from torch.nn.utils.rnn import pad_sequence
import gc

def collate_fn(batch):
    """
    batch: list of (features_tensor, label)
    features_tensor: [seq_len, feature_dim]
    """

    sequences, labels = zip(*batch)

    lengths = [len(seq) for seq in sequences]
    padded_seqs = nn.utils.rnn.pad_sequence(sequences, batch_first=True)
    
    return padded_seqs, torch.tensor(lengths), torch.tensor(labels)

class SpiralDataset(Dataset):
    def __init__(self, folder_path, transform=None):
        self.folder_path = folder_path
        self.files = [f for f in os.listdir(folder_path) if f.endswith('.txt')]
        self.transform = transform

    def __len__(self):
        return len(self.files)

    def __getitem__(self, idx):
        filename = self.files[idx]
        filepath = os.path.join(self.folder_path, filename)

        # Load data from file on demand
        data = np.loadtxt(filepath, delimiter=';')  # assuming whitespace or CSV format
        # data shape: [seq_len, num_features + maybe label col]
        # Assuming last column is not label, label is from filename

        # Extract features (all columns) â€” you can select columns if needed
        features = data[:, :5]  # x, y, z, pressure, timestamp (5 features)

        # Parse label from filename: example "PD_123.txt" -> label=1 else 0
        if "PD_" in filename:
            label = 1
        else:
            label = 0

        # Convert features to tensor
        features = torch.tensor(features, dtype=torch.float32)

        if self.transform:
            features = self.transform(features)

        return features, label

class LSTMFeatureExtractor(nn.Module):
    def __init__(self, input_dim, hidden_dim=128, num_layers=2, feature_dim=192):
        super().__init__()
        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True, bidirectional=True)
        self.fc = nn.Linear(hidden_dim * 2, feature_dim)  # bidirectional
    
    def forward(self, x, lengths):
        # pack padded sequence
        packed = nn.utils.rnn.pack_padded_sequence(x, lengths.cpu(), batch_first=True, enforce_sorted=False)
        packed_out, (h_n, c_n) = self.lstm(packed)
        # h_n shape: (num_layers*2, batch, hidden_dim)
        # concatenate last layer's forward and backward hidden states
        h_n = h_n.view(self.lstm.num_layers, 2, x.size(0), self.lstm.hidden_size)
        last_layer_h = h_n[-1]  # (2, batch, hidden_dim)
        h_cat = torch.cat([last_layer_h[0], last_layer_h[1]], dim=1)  # (batch, hidden_dim*2)
        features = self.fc(h_cat)  # (batch, feature_dim)
        return features

def extract_features(model, loader, device):
    model.eval()
    features_list = []
    labels_list = []
    with torch.no_grad():
        for x, lengths, y in loader:
            x, lengths = x.to(device), lengths.to(device)
            feats = model(x, lengths)
            features_list.append(feats.cpu())
            labels_list.append(y)
    features = torch.cat(features_list).numpy()
    labels = torch.cat(labels_list).numpy()
    return features, labels

def train_loop(model, loader, optimizer, criterion, device):
    model.train()
    total_loss = 0
    for x, lengths, y in loader:
        x, lengths, y = x.to(device), lengths.to(device), y.to(device)
        optimizer.zero_grad()
        features = model(x, lengths)
        loss = criterion(features, y)
        loss.backward()
        optimizer.step()
        total_loss += loss.item() * x.size(0)

        del x, lengths, y, loss
        torch.cuda.empty_cache()
        gc.collect()

    return total_loss / len(loader.dataset)

# Validation loop (no gradients)
def val_loop(model, loader, criterion, device):
    model.eval()
    total_loss = 0
    with torch.no_grad():
        for x, lengths, y in loader:
            x, lengths, y = x.to(device), lengths.to(device), y.to(device)
            features = model(x, lengths)
            loss = criterion(features, y)
            total_loss += loss.item() * x.size(0)
    return total_loss / len(loader.dataset)
    
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# train1_folder = './augmented_data/tabular'
train_folder = './preprocessed_data/tabular/train'

test_folder = './preprocessed_data/tabular/test'

full_train_dataset = SpiralDataset(train_folder)

train_size = int(0.8 * len(full_train_dataset))
val_size = len(full_train_dataset) - train_size
train_dataset, val_dataset = random_split(full_train_dataset, [train_size, val_size])

batch_size = 16

train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)
val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)

sample_seq, _ = full_train_dataset[0]
input_dim = sample_seq.shape[1]

model = LSTMFeatureExtractor(input_dim).to(device)
optimizer = optim.Adam(model.parameters(), lr=1e-3)
criterion = nn.CrossEntropyLoss()

best_val_loss = float('inf')
patience = 5
patience_counter = 0
num_epochs = 100


print("Starting training with early stopping on validation set...")
for epoch in range(num_epochs):
    train_loss = train_loop(model, train_loader, optimizer, criterion, device)
    val_loss = val_loop(model, val_loader, criterion, device)
    print(f"Epoch {epoch+1}: Train Loss={train_loss:.4f}, Val Loss={val_loss:.4f}")

    if val_loss < best_val_loss:
        best_val_loss = val_loss
        patience_counter = 0
        torch.save(model.state_dict(), "best_model.pth")
    else:
        patience_counter += 1
        if patience_counter >= patience:
            print("Early stopping triggered!")
            break

# Load best model
model.load_state_dict(torch.load("best_model.pth"))

print("Retraining on full training data...")
full_train_loader = DataLoader(full_train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)
model = LSTMFeatureExtractor(input_dim).to(device)
optimizer = optim.Adam(model.parameters(), lr=1e-3)

full_epochs = 50
for epoch in range(full_epochs):
    full_train_loss = train_loop(model, full_train_loader, optimizer, criterion, device)
    print(f"Full Train Epoch {epoch+1}: Loss={full_train_loss:.4f}")

# --- Extract and save features for train+val full set ---

test_dataset = SpiralDataset(test_folder)
test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)

train_features, train_labels = extract_features(model, full_train_loader, device)
pd.DataFrame(train_features).to_csv("train_features.csv", index=False)
pd.DataFrame(train_labels, columns=["label"]).to_csv("train_labels.csv", index=False)
print("Saved train features and labels.")

# --- Extract and save features for test set ---
test_features, test_labels = extract_features(model, test_loader, device)
pd.DataFrame(test_features).to_csv("test_features.csv", index=False)
pd.DataFrame(test_labels, columns=["label"]).to_csv("test_labels.csv", index=False)
print("Saved test features and labels.")
